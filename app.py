import streamlit as st
import pandas as pd
import io
import hashlib
import zipfile

# ==========================================
# é…ç½®ä¿¡æ¯
# ==========================================
APP_TITLE = "Zuma è¡¨æ ¼ç­›é€‰å·¥å…·"
APP_VERSION = "v2.3 (Default Rules)"
BUILD_DATE = "2026-01-12"

st.set_page_config(page_title=f"{APP_TITLE} {APP_VERSION}", layout="wide")
st.title(f"ğŸ“Š {APP_TITLE} (å·¥ä½œæµå¢å¼ºç‰ˆ)")
st.caption(f"Version: {APP_VERSION} | Build: {BUILD_DATE}")

# ==========================================
# 0. Session State åˆå§‹åŒ– (é¢„ç½® 5 ç»„è§„åˆ™)
# ==========================================
if 'filter_rules' not in st.session_state:
    # è¿™é‡Œæ ¹æ®ä½ çš„éœ€æ±‚é¢„è®¾äº† 5 ç»„å¸¸ç”¨æ¡ä»¶
    # é€»è¾‘ï¼šMin < x <= Max (Minè®¾ä¸º-1ä»¥åŒ…å«0)
    st.session_state.filter_rules = [
        # 1. Times 0 (0.0)
        {"id": 1, "min_t": -1.0, "max_t": 0.0, "min_l": -1, "max_l": 30},
        # 2. Times 0~1
        {"id": 2, "min_t": 0.0, "max_t": 1.0, "min_l": -1, "max_l": 30},
        # 3. Times 1~10
        {"id": 3, "min_t": 1.0, "max_t": 10.0, "min_l": -1, "max_l": 30},
        # 4. Times 10~100
        {"id": 4, "min_t": 10.0, "max_t": 100.0, "min_l": -1, "max_l": 30},
        # 5. Times 100~9999
        {"id": 5, "min_t": 100.0, "max_t": 9999.0, "min_l": -1, "max_l": 30},
    ]

def add_rule():
    new_id = len(st.session_state.filter_rules) + 1
    last_rule = st.session_state.filter_rules[-1]
    # æ–°å¢è§„åˆ™é»˜è®¤æ‰¿æ¥ä¸Šä¸€æ¡çš„ maxï¼ŒLauncher ä¿æŒ 0-30
    st.session_state.filter_rules.append(
        {"id": new_id, "min_t": last_rule['max_t'], "max_t": last_rule['max_t'] + 10.0, 
         "min_l": -1, "max_l": 30}
    )

def remove_rule():
    if len(st.session_state.filter_rules) > 1:
        st.session_state.filter_rules.pop()

# ==========================================
# 1. ä¾§è¾¹æ ï¼šæ‰¹é‡ç­›é€‰é…ç½®
# ==========================================
st.sidebar.header("1. æ‰¹é‡ç­›é€‰é…ç½®")
st.sidebar.info("ğŸ’¡ é»˜è®¤å·²åŠ è½½ 5 ç»„å¸¸ç”¨ç­›é€‰æ¡ä»¶ã€‚\nåŒºé—´é€»è¾‘ï¼šå·¦å¼€å³é—­ (Min < x â‰¤ Max)ã€‚")

col_btn1, col_btn2 = st.sidebar.columns(2)
col_btn1.button("â• å¢åŠ æ‹†åˆ†è§„åˆ™", on_click=add_rule, type="primary")
col_btn2.button("â– åˆ é™¤æœ€åä¸€æ¡", on_click=remove_rule)

st.sidebar.markdown("---")

# åŠ¨æ€æ¸²æŸ“è§„åˆ™
for i, rule in enumerate(st.session_state.filter_rules):
    idx = i + 1
    with st.sidebar.expander(f"ğŸ“‚ ä»»åŠ¡ {idx} (Times: {rule['min_t']}~{rule['max_t']})", expanded=False): 
        # é»˜è®¤æŠ˜å  expanded=False é¿å…ä¾§è¾¹æ å¤ªé•¿ï¼Œä½ å¯ä»¥æŒ‰éœ€å±•å¼€
        st.markdown(f"**åŒºé—´é€»è¾‘ï¼š({rule['min_t']} < Times â‰¤ {rule['max_t']}]**")
        
        c1, c2 = st.columns(2)
        rule['min_t'] = c1.number_input(f"Times > (Min)", value=float(rule['min_t']), step=0.1, key=f"t_min_{idx}")
        rule['max_t'] = c2.number_input(f"Times â‰¤ (Max)", value=float(rule['max_t']), step=0.1, key=f"t_max_{idx}")
        
        c3, c4 = st.columns(2)
        rule['min_l'] = c3.number_input(f"Launch > (Min)", value=int(rule['min_l']), step=1, key=f"l_min_{idx}")
        rule['max_l'] = c4.number_input(f"Launch â‰¤ (Max)", value=int(rule['max_l']), step=1, key=f"l_max_{idx}")

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šè¯»å–ä¸å¤„ç†
# ==========================================
def super_reader(file):
    """åº•å±‚è¯»å–é€»è¾‘"""
    df = None
    try:
        all_sheets = pd.read_excel(file, sheet_name=None)
        max_rows = 0
        for name, sheet_df in all_sheets.items():
            if len(sheet_df) > max_rows:
                max_rows = len(sheet_df)
                df = sheet_df
    except:
        pass
    
    if df is None:
        methods = [(pd.read_csv, {}), (pd.read_csv, {'encoding': 'utf-8'}), 
                   (pd.read_csv, {'encoding': 'gbk'}), (pd.read_csv, {'on_bad_lines': 'skip'})]
        for reader, kwargs in methods:
            file.seek(0)
            try:
                temp_df = reader(file, **kwargs)
                if not temp_df.empty: 
                    df = temp_df
                    break
            except: continue

    if df is not None and not df.empty:
        df.columns = df.columns.astype(str).str.strip()
        cols_to_drop = [c for c in df.columns if 'Unnamed' in c or c == '']
        if cols_to_drop: df.drop(columns=cols_to_drop, inplace=True)
            
    return df

@st.cache_data(show_spinner=False) 
def load_and_merge_files(files):
    """è¯»å–åˆå¹¶æ–‡ä»¶"""
    all_dfs = []
    for file in files:
        file.seek(0)
        df = super_reader(file)
        if df is not None and not df.empty:
            try:
                if 'Amount' in df.columns and 'LauncherNum' in df.columns:
                    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce').fillna(0)
                    df['Times'] = (df['Amount'] + 10000) / 10000
                    all_dfs.append(df)
            except:
                continue     
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return pd.DataFrame()

# ==========================================
# 3. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.header("2. ä¸Šä¼ æ•°æ®æ–‡ä»¶")
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼  Excel æˆ– CSV æ–‡ä»¶", 
    type=['xlsx', 'xls', 'csv'],
    accept_multiple_files=True
)

if uploaded_files:
    with st.spinner("æ­£åœ¨è¯»å–æˆ–ä»ç¼“å­˜åŠ è½½æ•°æ®..."):
        master_df = load_and_merge_files(uploaded_files)

    if not master_df.empty:
        # --- ç»Ÿè®¡é¢æ¿ ---
        st.markdown("### ğŸ“ˆ æ•°æ®å…¨è²Œç»Ÿè®¡")
        c1, c2, c3 = st.columns(3)
        c1.metric("ğŸ“¦ æ€»æ•°æ®è¡Œæ•°", f"{len(master_df):,} è¡Œ")
        c2.metric("âœ–ï¸ Times èŒƒå›´", f"{master_df['Times'].min():.2f} ~ {master_df['Times'].max():.2f}")
        c3.metric("ğŸš€ LauncherNum èŒƒå›´", f"{master_df['LauncherNum'].min()} ~ {master_df['LauncherNum'].max()}")
        st.divider()

        # --- æ‰¹é‡å¤„ç†é€»è¾‘ ---
        st.markdown(f"### ğŸš€ æ‰¹é‡æ‹†åˆ†å¤„ç† (å…± {len(st.session_state.filter_rules)} ä¸ªä»»åŠ¡)")
        
        if st.button("ğŸ‘‰ å¼€å§‹æ‰¹é‡æ‹†åˆ†å¹¶æ‰“åŒ…ä¸‹è½½", type="primary"):
            
            results_buffer = io.BytesIO()
            processed_logs = []
            total_files_generated = 0

            with zipfile.ZipFile(results_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                progress_bar = st.progress(0)
                
                for i, rule in enumerate(st.session_state.filter_rules):
                    idx = i + 1
                    t_min, t_max = rule['min_t'], rule['max_t']
                    l_min, l_max = rule['min_l'], rule['max_l']
                    
                    # 1. ç­›é€‰
                    filtered_df = master_df[
                        (master_df['Times'] > t_min) & 
                        (master_df['Times'] <= t_max) & 
                        (master_df['LauncherNum'] > l_min) & 
                        (master_df['LauncherNum'] <= l_max)
                    ].copy()
                    
                    # æ–‡ä»¶åä¼˜åŒ–ï¼šæ˜¾ç¤º Times èŒƒå›´
                    file_name = f"File{idx}_Times_{t_min}-{t_max}.csv"
                    
                    if not filtered_df.empty:
                        # 2. ç”Ÿæˆ MD5
                        filtered_df['Row_MD5'] = filtered_df.apply(
                            lambda row: hashlib.md5("".join(row.astype(str).values).encode('utf-8')).hexdigest(), axis=1
                        )

                        # 3. ç”Ÿæˆ Batch_ID (æ™ºèƒ½å¹³å‡å€¼é€»è¾‘)
                        real_data_mean = filtered_df['Times'].mean()
                        prefix_str = str(int(round(real_data_mean * 100))).zfill(6)
                        
                        WIDTH_INDEX = 6
                        filtered_df['Batch_ID'] = [f"{prefix_str}{str(k+1).zfill(WIDTH_INDEX)}" for k in range(len(filtered_df))]

                        # 4. åˆ—é‡æ’
                        cols = list(filtered_df.columns)
                        priority = ['Batch_ID', 'Row_MD5']
                        others = [c for c in cols if c not in priority]
                        if 'Times' in others and 'Amount' in others:
                            others.remove('Times')
                            others.insert(others.index('Amount') + 1, 'Times')
                        
                        final_df = filtered_df[priority + others]
                        
                        # 5. å†™å…¥ ZIP
                        csv_data = final_df.to_csv(index=False).encode('utf-8-sig')
                        zf.writestr(file_name, csv_data)
                        
                        processed_logs.append({
                            "ä»»åŠ¡": f"ä»»åŠ¡ {idx}",
                            "æ–‡ä»¶å": file_name, 
                            "çŠ¶æ€": "âœ… æˆåŠŸ", 
                            "è¡Œæ•°": len(final_df),
                            "IDå‰ç¼€": prefix_str
                        })
                        total_files_generated += 1
                    else:
                        processed_logs.append({
                            "ä»»åŠ¡": f"ä»»åŠ¡ {idx}",
                            "æ–‡ä»¶å": file_name, 
                            "çŠ¶æ€": "âš ï¸ è·³è¿‡ (æ— æ•°æ®)", 
                            "è¡Œæ•°": 0, 
                            "IDå‰ç¼€": "-"
                        })
                    
                    progress_bar.progress((i + 1) / len(st.session_state.filter_rules))

            # ç»“æœå±•ç¤º
            if total_files_generated > 0:
                st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {total_files_generated} ä¸ªæ–‡ä»¶ã€‚")
                st.table(pd.DataFrame(processed_logs))
                
                st.download_button(
                    label="ğŸ“¦ ç‚¹å‡»ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ (ZIP)",
                    data=results_buffer.getvalue(),
                    file_name=f"Batch_Processed_{BUILD_DATE}.zip",
                    mime="application/zip"
                )
            else:
                st.error("æ‰€æœ‰ç­›é€‰ç»“æœä¸ºç©ºã€‚")
                st.table(pd.DataFrame(processed_logs))

    else:
        st.error("æœªèƒ½è¯»å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
else:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ æ–‡ä»¶ã€‚")