import streamlit as st
import pandas as pd
import io
import hashlib
import zipfile  # æ–°å¢ï¼šç”¨äºæ‰“åŒ…å¤šä¸ªæ–‡ä»¶

# ==========================================
# é…ç½®ä¿¡æ¯
# ==========================================
APP_TITLE = "Zuma è¡¨æ ¼ç­›é€‰å·¥å…·"
APP_VERSION = "v2.0 (Batch Process)"
BUILD_DATE = "2026-01-12"

st.set_page_config(page_title=f"{APP_TITLE} {APP_VERSION}", layout="wide")
st.title(f"ğŸ“Š {APP_TITLE} (æ‰¹é‡æ‹†åˆ†ç‰ˆ)")
st.caption(f"Version: {APP_VERSION} | Build: {BUILD_DATE}")

# ==========================================
# 0. Session State åˆå§‹åŒ– (ç”¨äºç®¡ç†å¤šç»„ç­›é€‰æ¡ä»¶)
# ==========================================
if 'filter_rules' not in st.session_state:
    # é»˜è®¤åˆå§‹åŒ–ä¸€ç»„è§„åˆ™
    st.session_state.filter_rules = [
        {"id": 1, "min_t": 0.0, "max_t": 1000.0, "min_l": 0, "max_l": 100}
    ]

# è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ æ–°è§„åˆ™
def add_rule():
    new_id = len(st.session_state.filter_rules) + 1
    st.session_state.filter_rules.append(
        {"id": new_id, "min_t": 0.0, "max_t": 100.0, "min_l": 0, "max_l": 100}
    )

# è¾…åŠ©å‡½æ•°ï¼šåˆ é™¤æœ€åä¸€æ¡è§„åˆ™
def remove_rule():
    if len(st.session_state.filter_rules) > 1:
        st.session_state.filter_rules.pop()

# ==========================================
# 1. ä¾§è¾¹æ ï¼šæ‰¹é‡ç­›é€‰é…ç½®
# ==========================================
st.sidebar.header("1. æ‰¹é‡ç­›é€‰é…ç½®")
st.sidebar.info("ğŸ’¡ ä½ å¯ä»¥æ·»åŠ å¤šç»„æ¡ä»¶ï¼Œç¨‹åºå°†ä¸€æ¬¡æ€§æ‹†åˆ†å‡ºå¯¹åº”çš„å¤šä¸ªæ–‡ä»¶ã€‚")

# è§„åˆ™ç®¡ç†æŒ‰é’®
col_btn1, col_btn2 = st.sidebar.columns(2)
col_btn1.button("â• å¢åŠ æ‹†åˆ†è§„åˆ™", on_click=add_rule, type="primary")
col_btn2.button("â– åˆ é™¤æœ€åä¸€æ¡", on_click=remove_rule)

st.sidebar.markdown("---")

# åŠ¨æ€æ¸²æŸ“æ‰€æœ‰è§„åˆ™çš„è¾“å…¥æ¡†
# æ³¨æ„ï¼šåœ¨å¾ªç¯ä¸­ç”Ÿæˆç»„ä»¶å¿…é¡»æŒ‡å®šå”¯ä¸€çš„ key
for i, rule in enumerate(st.session_state.filter_rules):
    idx = i + 1
    with st.sidebar.expander(f"ğŸ“‚ æ–‡ä»¶ {idx} é…ç½® (Rule {idx})", expanded=True):
        c1, c2 = st.columns(2)
        rule['min_t'] = c1.number_input(f"Times Min", value=rule['min_t'], step=0.1, key=f"t_min_{idx}")
        rule['max_t'] = c2.number_input(f"Times Max", value=rule['max_t'], step=0.1, key=f"t_max_{idx}")
        
        c3, c4 = st.columns(2)
        rule['min_l'] = c3.number_input(f"Launch Min", value=rule['min_l'], step=1, key=f"l_min_{idx}")
        rule['max_l'] = c4.number_input(f"Launch Max", value=rule['max_l'], step=1, key=f"l_max_{idx}")

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šè¯»å–ä¸å¤„ç† (å¸¦ç¼“å­˜ + æ¸…æ´—)
# ==========================================
def super_reader(file):
    """åº•å±‚è¯»å–é€»è¾‘"""
    df = None
    try:
        all_sheets = pd.read_excel(file, sheet_name=None)
        max_rows = 0
        for name, sheet_df in all_sheets.items():
            if len(sheet_df) > max_rows:
                max_rows = len(sheet_df)
                df = sheet_df
    except:
        pass
    
    if df is None:
        methods = [(pd.read_csv, {}), (pd.read_csv, {'encoding': 'utf-8'}), 
                   (pd.read_csv, {'encoding': 'gbk'}), (pd.read_csv, {'on_bad_lines': 'skip'})]
        for reader, kwargs in methods:
            file.seek(0)
            try:
                temp_df = reader(file, **kwargs)
                if not temp_df.empty: 
                    df = temp_df
                    break
            except: continue

    if df is not None and not df.empty:
        df.columns = df.columns.astype(str).str.strip()
        cols_to_drop = [c for c in df.columns if 'Unnamed' in c or c == '']
        if cols_to_drop: df.drop(columns=cols_to_drop, inplace=True)
            
    return df

@st.cache_data(show_spinner=False) 
def load_and_merge_files(files):
    """è¯»å–åˆå¹¶æ–‡ä»¶ (ç¼“å­˜åŠ é€Ÿ)"""
    all_dfs = []
    for file in files:
        file.seek(0)
        df = super_reader(file)
        if df is not None and not df.empty:
            try:
                if 'Amount' in df.columns and 'LauncherNum' in df.columns:
                    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce').fillna(0)
                    df['Times'] = (df['Amount'] + 10000) / 10000
                    all_dfs.append(df)
            except:
                continue     
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return pd.DataFrame()

# ==========================================
# 3. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.header("2. ä¸Šä¼ æ•°æ®æ–‡ä»¶")
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼  Excel æˆ– CSV æ–‡ä»¶", 
    type=['xlsx', 'xls', 'csv'],
    accept_multiple_files=True
)

if uploaded_files:
    # è¯»å–è¿‡ç¨‹ (å¸¦ç¼“å­˜)
    with st.spinner("æ­£åœ¨è¯»å–æˆ–ä»ç¼“å­˜åŠ è½½æ•°æ®..."):
        master_df = load_and_merge_files(uploaded_files)

    if not master_df.empty:
        # --- ç»Ÿè®¡é¢æ¿ ---
        st.markdown("### ğŸ“ˆ æ•°æ®å…¨è²Œç»Ÿè®¡")
        c1, c2, c3 = st.columns(3)
        c1.metric("ğŸ“¦ æ€»æ•°æ®è¡Œæ•°", f"{len(master_df):,} è¡Œ")
        c2.metric("âœ–ï¸ Times èŒƒå›´", f"{master_df['Times'].min():.2f} ~ {master_df['Times'].max():.2f}")
        c3.metric("ğŸš€ LauncherNum èŒƒå›´", f"{master_df['LauncherNum'].min()} ~ {master_df['LauncherNum'].max()}")
        st.divider()

        # --- æ‰¹é‡å¤„ç†é€»è¾‘ ---
        st.markdown(f"### ğŸš€ æ‰¹é‡æ‹†åˆ†å¤„ç† (å½“å‰å…± {len(st.session_state.filter_rules)} ä¸ªä»»åŠ¡)")
        
        if st.button("ğŸ‘‰ å¼€å§‹æ‰¹é‡æ‹†åˆ†å¹¶æ‰“åŒ…ä¸‹è½½", type="primary"):
            
            results_buffer = io.BytesIO() # ç”¨äºå­˜æ”¾ ZIP æ–‡ä»¶çš„å†…å­˜
            processed_logs = [] # ç”¨äºè®°å½•å¤„ç†ç»“æœæ—¥å¿—
            total_files_generated = 0

            # åˆ›å»º ZIP æ–‡ä»¶
            with zipfile.ZipFile(results_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                
                progress_bar = st.progress(0)
                
                # éå†æ‰€æœ‰è§„åˆ™
                for i, rule in enumerate(st.session_state.filter_rules):
                    idx = i + 1
                    t_min, t_max = rule['min_t'], rule['max_t']
                    l_min, l_max = rule['min_l'], rule['max_l']
                    
                    # 1. ç­›é€‰
                    filtered_df = master_df[
                        (master_df['Times'] >= t_min) & 
                        (master_df['Times'] <= t_max) & 
                        (master_df['LauncherNum'] >= l_min) & 
                        (master_df['LauncherNum'] <= l_max)
                    ].copy()
                    
                    file_name = f"File{idx}_Times_{t_min}-{t_max}_L_{l_min}-{l_max}.csv"
                    
                    if not filtered_df.empty:
                        # 2. ç”Ÿæˆ MD5
                        filtered_df['Row_MD5'] = filtered_df.apply(
                            lambda row: hashlib.md5("".join(row.astype(str).values).encode('utf-8')).hexdigest(), axis=1
                        )

                        # 3. ç”Ÿæˆ Batch_ID (åŸºäºå½“å‰è§„åˆ™çš„ Min/Max)
                        avg_val = (t_min + t_max) / 2
                        prefix_str = str(int(round(avg_val * 100))).zfill(6)
                        filtered_df['Batch_ID'] = [f"{prefix_str}{str(k+1).zfill(6)}" for k in range(len(filtered_df))]

                        # 4. åˆ—é‡æ’
                        cols = list(filtered_df.columns)
                        priority = ['Batch_ID', 'Row_MD5']
                        others = [c for c in cols if c not in priority]
                        if 'Times' in others and 'Amount' in others:
                            others.remove('Times')
                            others.insert(others.index('Amount') + 1, 'Times')
                        
                        final_df = filtered_df[priority + others]
                        
                        # 5. å†™å…¥ ZIP
                        csv_data = final_df.to_csv(index=False).encode('utf-8-sig')
                        zf.writestr(file_name, csv_data)
                        
                        processed_logs.append({"æ–‡ä»¶": file_name, "çŠ¶æ€": "âœ… æˆåŠŸ", "è¡Œæ•°": len(final_df)})
                        total_files_generated += 1
                    else:
                        processed_logs.append({"æ–‡ä»¶": file_name, "çŠ¶æ€": "âš ï¸ è·³è¿‡ (æ— æ•°æ®)", "è¡Œæ•°": 0})
                    
                    progress_bar.progress((i + 1) / len(st.session_state.filter_rules))

            # ç»“æœå±•ç¤º
            if total_files_generated > 0:
                st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {total_files_generated} ä¸ªæ–‡ä»¶ã€‚")
                
                # å±•ç¤ºæ—¥å¿—è¡¨æ ¼
                st.table(pd.DataFrame(processed_logs))
                
                # æä¾› ZIP ä¸‹è½½
                st.download_button(
                    label="ğŸ“¦ ç‚¹å‡»ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ (ZIPå‹ç¼©åŒ…)",
                    data=results_buffer.getvalue(),
                    file_name=f"Batch_Processed_{BUILD_DATE}.zip",
                    mime="application/zip"
                )
            else:
                st.error("æ‰€æœ‰ç­›é€‰æ¡ä»¶çš„ç­›é€‰ç»“æœå‡ä¸ºç©ºï¼Œæœªç”Ÿæˆä»»ä½•æ–‡ä»¶ã€‚")
                st.table(pd.DataFrame(processed_logs))

    else:
        st.error("æœªèƒ½è¯»å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
else:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ æ–‡ä»¶ã€‚")