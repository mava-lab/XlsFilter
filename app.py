import streamlit as st
import pandas as pd
import io
import hashlib
import zipfile
import gc  # å¼•å…¥åƒåœ¾å›æ”¶æœºåˆ¶ï¼Œå¼ºåˆ¶é‡Šæ”¾å†…å­˜

# ==========================================
# é…ç½®ä¿¡æ¯
# ==========================================
APP_TITLE = "Zuma è¡¨æ ¼ç­›é€‰å·¥å…·"
APP_VERSION = "v3.0 (Memory Safe)"
BUILD_DATE = "2026-01-12"

st.set_page_config(page_title=f"{APP_TITLE} {APP_VERSION}", layout="wide")
st.title(f"ğŸ“Š {APP_TITLE} (å¤§æ–‡ä»¶é˜²å´©æºƒç‰ˆ)")
st.caption(f"Version: {APP_VERSION} | Build: {BUILD_DATE}")
st.info("ğŸ’¡ æ­¤ç‰ˆæœ¬é‡‡ç”¨**æµå¼å¤„ç†**æŠ€æœ¯ï¼šæ–‡ä»¶é€ä¸ªè¯»å–ã€å¤„ç†å¹¶é‡Šæ”¾å†…å­˜ï¼Œä¸å†åˆå¹¶å¤§è¡¨ã€‚é€‚åˆå¤„ç†æ•°ç™¾å…†ç”šè‡³ GB çº§æ•°æ®ã€‚")

# ==========================================
# 0. Session State åˆå§‹åŒ– (é»˜è®¤ 5 ç»„è§„åˆ™)
# ==========================================
if 'filter_rules' not in st.session_state:
    st.session_state.filter_rules = [
        {"id": 1, "min_t": -1.0, "max_t": 0.0, "min_l": -1, "max_l": 30},
        {"id": 2, "min_t": 0.0, "max_t": 1.0, "min_l": -1, "max_l": 30},
        {"id": 3, "min_t": 1.0, "max_t": 10.0, "min_l": -1, "max_l": 30},
        {"id": 4, "min_t": 10.0, "max_t": 100.0, "min_l": -1, "max_l": 30},
        {"id": 5, "min_t": 100.0, "max_t": 9999.0, "min_l": -1, "max_l": 30},
    ]

def add_rule():
    new_id = len(st.session_state.filter_rules) + 1
    last_rule = st.session_state.filter_rules[-1]
    st.session_state.filter_rules.append(
        {"id": new_id, "min_t": last_rule['max_t'], "max_t": last_rule['max_t'] + 10.0, 
         "min_l": -1, "max_l": 30}
    )

def remove_rule():
    if len(st.session_state.filter_rules) > 1:
        st.session_state.filter_rules.pop()

# ==========================================
# 1. ä¾§è¾¹æ é…ç½®
# ==========================================
st.sidebar.header("1. æ‰¹é‡ç­›é€‰é…ç½®")
col_btn1, col_btn2 = st.sidebar.columns(2)
col_btn1.button("â• å¢åŠ æ‹†åˆ†è§„åˆ™", on_click=add_rule, type="primary")
col_btn2.button("â– åˆ é™¤æœ€åä¸€æ¡", on_click=remove_rule)
st.sidebar.markdown("---")

for i, rule in enumerate(st.session_state.filter_rules):
    idx = i + 1
    with st.sidebar.expander(f"ğŸ“‚ ä»»åŠ¡ {idx} (Times: {rule['min_t']}~{rule['max_t']})", expanded=False): 
        st.markdown(f"**åŒºé—´: ({rule['min_t']} < Times â‰¤ {rule['max_t']}]**")
        c1, c2 = st.columns(2)
        rule['min_t'] = c1.number_input(f"Times Min", value=float(rule['min_t']), step=0.1, key=f"t_min_{idx}")
        rule['max_t'] = c2.number_input(f"Times Max", value=float(rule['max_t']), step=0.1, key=f"t_max_{idx}")
        c3, c4 = st.columns(2)
        rule['min_l'] = c3.number_input(f"Launch Min", value=int(rule['min_l']), step=1, key=f"l_min_{idx}")
        rule['max_l'] = c4.number_input(f"Launch Max", value=int(rule['max_l']), step=1, key=f"l_max_{idx}")

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šå•æ–‡ä»¶è¯»å– (ä¸ç¼“å­˜ï¼Œçœå†…å­˜)
# ==========================================
def read_single_file(file):
    """è¯»å–å•ä¸ªæ–‡ä»¶ï¼Œæ¸…æ´—åˆ—åï¼Œè®¡ç®—Timesï¼Œç„¶åè¿”å›DF"""
    df = None
    try:
        all_sheets = pd.read_excel(file, sheet_name=None)
        max_rows = 0
        for name, sheet_df in all_sheets.items():
            if len(sheet_df) > max_rows:
                max_rows = len(sheet_df)
                df = sheet_df
    except:
        pass
    
    if df is None:
        methods = [(pd.read_csv, {}), (pd.read_csv, {'encoding': 'utf-8'}), (pd.read_csv, {'encoding': 'gbk'}), (pd.read_csv, {'on_bad_lines': 'skip'})]
        for reader, kwargs in methods:
            file.seek(0)
            try:
                temp_df = reader(file, **kwargs)
                if not temp_df.empty: 
                    df = temp_df
                    break
            except: continue

    if df is not None and not df.empty:
        # æ¸…æ´—
        df.columns = df.columns.astype(str).str.strip()
        cols_to_drop = [c for c in df.columns if 'Unnamed' in c or c == '']
        if cols_to_drop: df.drop(columns=cols_to_drop, inplace=True)
        
        # è®¡ç®— Times
        if 'Amount' in df.columns:
            df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce').fillna(0)
            df['Times'] = (df['Amount'] + 10000) / 10000
        else:
            return None # ç¼ºå°‘å…³é”®åˆ—
            
    return df

# ==========================================
# 3. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.header("2. ä¸Šä¼ æ•°æ®æ–‡ä»¶")
# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¢åŠ äº† file_uploader çš„æç¤ºï¼Œå»ºè®®ç”¨æˆ·ä¿®æ”¹ config å…è®¸å¤§æ–‡ä»¶
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼  Excel æˆ– CSV æ–‡ä»¶ (æ”¯æŒå¤šæ–‡ä»¶ï¼Œå»ºè®®æ€»å¤§å° < 500MB)", 
    type=['xlsx', 'xls', 'csv'],
    accept_multiple_files=True
)

if uploaded_files:
    st.success(f"å·²æ¥æ”¶ {len(uploaded_files)} ä¸ªæ–‡ä»¶ã€‚å‡†å¤‡å°±ç»ªã€‚")
    
    # ç§»é™¤åŸæœ¬çš„â€œé¢„è¯»å–â€ç»Ÿè®¡æ­¥éª¤ï¼Œå› ä¸ºè¿™ä¼šæ¶ˆè€—å¤§é‡å†…å­˜
    # ç›´æ¥è¿›å…¥å¤„ç†ç¯èŠ‚
    
    if st.button("ğŸ‘‰ å¼€å§‹æµå¼å¤„ç†å¹¶æ‰“åŒ… (Memory Safe)", type="primary"):
        
        # ç»“æœå®¹å™¨ï¼šæˆ‘ä»¬ç”¨ä¸€ä¸ªå­—å…¸æ¥æš‚å­˜æ¯ä¸ªè§„åˆ™ç­›é€‰å‡ºçš„æ•°æ®ç‰‡æ®µ
        # key = rule_id, value = list of dataframes
        rule_results = {rule['id']: [] for rule in st.session_state.filter_rules}
        
        # è¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_files = len(uploaded_files)
        
        # === æ ¸å¿ƒå¾ªç¯ï¼šæ–‡ä»¶é€ä¸ªå¤„ç† ===
        for i, file in enumerate(uploaded_files):
            status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_files} ä¸ªæ–‡ä»¶: {file.name} ...")
            
            # 1. è¯»å…¥å†…å­˜
            current_df = read_single_file(file)
            
            if current_df is not None and not current_df.empty and 'LauncherNum' in current_df.columns:
                
                # 2. éå†æ‰€æœ‰è§„åˆ™ï¼Œå¯¹å½“å‰è¿™ä¸ªæ–‡ä»¶è¿›è¡Œâ€œåˆ‡åˆ†â€
                for rule in st.session_state.filter_rules:
                    t_min, t_max = rule['min_t'], rule['max_t']
                    l_min, l_max = rule['min_l'], rule['max_l']
                    
                    # ç­›é€‰ç‰‡æ®µ
                    subset = current_df[
                        (current_df['Times'] > t_min) & 
                        (current_df['Times'] <= t_max) & 
                        (current_df['LauncherNum'] > l_min) & 
                        (current_df['LauncherNum'] <= l_max)
                    ].copy()
                    
                    if not subset.empty:
                        # å°†ç‰‡æ®µå­˜å…¥å¯¹åº”è§„åˆ™çš„åˆ—è¡¨ä¸­
                        rule_results[rule['id']].append(subset)
            
            # 3. ã€å…³é”®ã€‘é‡Šæ”¾å†…å­˜
            del current_df
            gc.collect() # å¼ºåˆ¶é€šçŸ¥ Python å›æ”¶å†…å­˜
            
            progress_bar.progress((i + 1) / total_files)

        status_text.text("æ–‡ä»¶éå†å®Œæˆï¼Œæ­£åœ¨åˆå¹¶ç»“æœå¹¶ç”Ÿæˆ ID...")
        
        # === åˆå¹¶ç»“æœå¹¶ç”Ÿæˆ ZIP ===
        results_buffer = io.BytesIO()
        processed_logs = []
        files_count = 0
        
        with zipfile.ZipFile(results_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            
            # éå†æ¯ä¸ªè§„åˆ™çš„æ”¶é›†ç»“æœ
            for i, rule in enumerate(st.session_state.filter_rules):
                rule_id = rule['id']
                df_list = rule_results[rule_id]
                
                # æ–‡ä»¶å
                file_name = f"File{i+1}_Times_{rule['min_t']}-{rule['max_t']}.csv"
                
                if df_list:
                    # åˆå¹¶è¯¥è§„åˆ™ä¸‹çš„æ‰€æœ‰ç¢ç‰‡
                    final_df = pd.concat(df_list, ignore_index=True)
                    
                    # --- ç”Ÿæˆåˆ—é€»è¾‘ (ä¸ä¹‹å‰ä¸€è‡´) ---
                    # MD5
                    final_df['Row_MD5'] = final_df.apply(
                        lambda row: hashlib.md5("".join(row.astype(str).values).encode('utf-8')).hexdigest(), axis=1
                    )
                    
                    # Batch_ID (æ™ºèƒ½å‡å€¼)
                    real_mean = final_df['Times'].mean()
                    prefix = str(int(round(real_mean * 100))).zfill(6)
                    final_df['Batch_ID'] = [f"{prefix}{str(k+1).zfill(6)}" for k in range(len(final_df))]
                    
                    # æ’åº
                    cols = list(final_df.columns)
                    prio = ['Batch_ID', 'Row_MD5']
                    others = [c for c in cols if c not in prio]
                    if 'Times' in others and 'Amount' in others:
                        others.remove('Times')
                        others.insert(others.index('Amount')+1, 'Times')
                    final_df = final_df[prio + others]
                    
                    # å†™å…¥ ZIP
                    zf.writestr(file_name, final_df.to_csv(index=False).encode('utf-8-sig'))
                    
                    processed_logs.append({
                        "ä»»åŠ¡": f"ä»»åŠ¡ {i+1}", 
                        "æ–‡ä»¶å": file_name, 
                        "çŠ¶æ€": "âœ… æˆåŠŸ", 
                        "è¡Œæ•°": len(final_df),
                        "IDå‰ç¼€": prefix
                    })
                    files_count += 1
                    
                    # å†æ¬¡é‡Šæ”¾å†…å­˜
                    del final_df
                    del df_list
                    gc.collect()
                else:
                    processed_logs.append({
                        "ä»»åŠ¡": f"ä»»åŠ¡ {i+1}", 
                        "æ–‡ä»¶å": file_name, 
                        "çŠ¶æ€": "âš ï¸ æ— æ•°æ®", 
                        "è¡Œæ•°": 0,
                        "IDå‰ç¼€": "-"
                    })

        st.success("å…¨éƒ¨å¤„ç†å®Œæˆï¼")
        st.table(pd.DataFrame(processed_logs))
        
        if files_count > 0:
            st.download_button(
                label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰ç»“æœ (ZIP)",
                data=results_buffer.getvalue(),
                file_name=f"Batch_Results_{BUILD_DATE}.zip",
                mime="application/zip"
            )

else:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ æ–‡ä»¶ã€‚å»ºè®®å•ä¸ªæ–‡ä»¶ä¸è¦è¶…è¿‡ 200MBã€‚")